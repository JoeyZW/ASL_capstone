{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9562c057-4db9-4dd2-9916-801dd4596388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 15:18:37.274066: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-16 15:18:38.127731: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-02-16 15:18:38.127845: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-02-16 15:18:38.127856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "# import tensorflow_text as text\n",
    "import keras\n",
    "\n",
    "from keras import Input\n",
    "from keras.layers import TextVectorization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.layers import (\n",
    "    GRU,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    Lambda,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836512ba-b4ae-4d4e-abb9-3a31012b5683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 15:18:40.871069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-16 15:18:40.883772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-16 15:18:40.885582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-16 15:18:40.887942: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-16 15:18:40.889194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-16 15:18:40.890938: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-16 15:18:40.892639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-16 15:18:41.626879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-16 15:18:41.628819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-16 15:18:41.630459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-16 15:18:41.632023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13582 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d503f6e-4016-4dcb-aa32-4f138616f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "# data = pd.read_csv(\"../data/patent_rand100k.csv\")\n",
    "# data = pd.read_csv(\"../data/patent_evan-100K.csv\")#, quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "url = 'https://drive.google.com/file/d/16iYEWb1xCiuR5_TM47RVZyzLFwD7gZnT/view?usp=share_link'\n",
    "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
    "df = pd.read_csv(path)\n",
    "#shuffle data\n",
    "data = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6def4e3f-7bbb-4500-88d1-57c2a7060098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data.groupby('ipc_section').count()\n",
    "# dataSort = data.query('(ipc_section == \"A\")').sample(n=1000)\n",
    "# dataSort = pd.concat([dataSort,data.query('(ipc_section == \"B\")').sample(n=1000)])\n",
    "# dataSort = pd.concat([dataSort,data.query('(ipc_section == \"C\")').sample(n=1000)])\n",
    "# dataSort = pd.concat([dataSort,data.query('(ipc_section == \"D\")').sample(n=1000)])\n",
    "# dataSort = pd.concat([dataSort,data.query('(ipc_section == \"E\")').sample(n=1000)])\n",
    "# dataSort = pd.concat([dataSort,data.query('(ipc_section == \"F\")').sample(n=1000)])\n",
    "# dataSort = pd.concat([dataSort,data.query('(ipc_section == \"G\")').sample(n=1000)])\n",
    "# dataSort = pd.concat([dataSort,data.query('(ipc_section == \"H\")').sample(n=1000)])\n",
    "# # data=data.sample(50000)\n",
    "# # data=dataSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df50fd1-fc17-40f4-9ab9-3a07d5c562ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f33fb9e-fd82-4347-b5a9-1390c9725bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot encode target\n",
    "# classes = [[0,1,2,3,4,5,6,7][source] for source in dataSort.ipc_section]\n",
    "classes = [{\"A\":0,\"B\":1,\"C\":2,\"D\":3,\"E\":4,\"F\":5,\"G\":6,\"H\":7}[source] for source in data.ipc_section]\n",
    "target = to_categorical(classes)\n",
    "#print target\n",
    "# target[2002]\n",
    "len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55a399a1-1c1f-4976-ae34-3a6a01d01679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"modeltest\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " InputString (InputLayer)    [(None, 512)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 4104      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,104\n",
      "Trainable params: 4,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCHSIZE = 50\n",
    "\n",
    "inputs = Input(shape=(512,),dtype=tf.float32,name=\"InputString\")\n",
    "# x = Dense(256,activation=\"relu\")(inputs)\n",
    "# x = Dense(256,activation=\"relu\",kernel_regularizer=tf.keras.regularizers.L2(0.02))(inputs)\n",
    "# x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = Dense(8,activation=\"softmax\")(inputs)\n",
    "\n",
    "model = keras.Model(inputs=inputs,outputs=outputs,name=\"modeltest\")\n",
    "model.summary()\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",metrics=[\"Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6920fe9-fb83-4c69-9fea-a0dc9817c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelhistory = model.fit(\n",
    "    embed(data.abstract[:20000]),\n",
    "    target[:20000], \n",
    "    epochs=50,\n",
    "    batch_size=BATCHSIZE, \n",
    "    validation_split=0.2\n",
    "    # validation_data=(data.text[6000:],target[6000:]),\n",
    "    # callbacks=[EarlyStopping(patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8311d-8049-4276-912e-31489bcc3e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(modelhistory.history)[[\"loss\",\"val_loss\"]].plot(title=\"BS=\"+ str(BATCHSIZE) +\" | USE | 256 dense\")\n",
    "pd.DataFrame(modelhistory.history)[[\"Accuracy\",\"val_Accuracy\"]].plot(title=\"BS=\"+ str(BATCHSIZE) +\" | USE | 256 dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ac684-242c-41cd-97e1-0927b620936b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9441415-9cd9-4340-b23d-5f1f36311e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
